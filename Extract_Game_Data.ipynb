{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-23T11:43:48.734462400Z",
     "start_time": "2024-07-23T11:43:48.727946100Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extraction Info\n",
    "\n",
    "### raw.csv\n",
    "for each benchmark there is the raw.csv containing all metrics collected.\n",
    "from this file, the following information can be extracted for each episode:\n",
    "- game\n",
    "- model\n",
    "- episode\n",
    "- experiment\n",
    "- Aborted\n",
    "- Lose\n",
    "- Success\n",
    "\n",
    "### requests.json\n",
    "For each benchmark > model > game > experiment > episode there is a requests file containing the requests sent to the models and the received answers\n",
    "following information can be extracted for each episode\n",
    "- inputs\n",
    "- responses\n",
    "\n",
    "### instance.json\n",
    "For each benchmark > model > game > experiment > episode there is a instance.json file containing metadata for the episode\n",
    "e.g. for taboo it is the target word along with the taboo words or for wordle it is the target word along with a clue for the word.\n",
    "the metadata will be extracted completely and stored as an additional column.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5f48ea0a6dee261"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def group_raw_csv(data: pd.DataFrame, columns_to_keep: list[str]) -> pd.DataFrame:\n",
    "    df: pd.DataFrame = data.pivot_table(\n",
    "        index=['game', 'model', 'experiment', 'episode'],\n",
    "        columns=['metric'],\n",
    "        values='value'\n",
    "    ).reset_index()\n",
    "\n",
    "    columns_to_drop: list = [column for column in list(df.keys()) if column not in columns_to_keep]\n",
    "    return df.drop(columns=columns_to_drop, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T11:43:56.374972900Z",
     "start_time": "2024-07-23T11:43:56.367976100Z"
    }
   },
   "id": "5213b54a7607a69f"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def prepare_requests_json(path: str, input_parser: callable, output_parser: callable, model_name: str) -> dict:\n",
    "    data: pd.DataFrame = pd.read_json(path)\n",
    "    interactions: dict = {\n",
    "        'requests': [],\n",
    "        'responses': []\n",
    "    }\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        try: \n",
    "            interactions['requests'].append(input_parser(row.manipulated_prompt_obj))\n",
    "            interactions['responses'].append(output_parser(row.raw_response_obj))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(model_name)\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        assert len(interactions['requests']) == len(interactions['responses'])\n",
    "    except AssertionError as e:\n",
    "        print(e)\n",
    "        print('length missmatch between requests and responses')\n",
    "        print(model_name)\n",
    "        return\n",
    "\n",
    "        \n",
    "    return interactions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T12:11:39.170366600Z",
     "start_time": "2024-07-23T12:11:39.148370400Z"
    }
   },
   "id": "a80dd8f2d25fbc1e"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def prepare_instance_data(path: str) -> dict:\n",
    "    with open(path, 'r') as f:\n",
    "        data: dict = json.load(f)\n",
    "        return data\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T12:07:13.210287600Z",
     "start_time": "2024-07-23T12:07:13.179289400Z"
    }
   },
   "id": "7d749f89d5d8ce20"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "benchmark_versions_old: list = ['v0.9', 'v1.0']\n",
    "interaction_response_token_old: list = ['completion', 'response']\n",
    "lookup_response_token_old: dict = {key: val for key, val in zip(benchmark_versions_old, interaction_response_token_old)}\n",
    "benchmark_versions_new: list = ['v1.5', 'v1.5_quantized','v1.6','v1.6_backends','v1.6_quantized',]\n",
    "columns_to_keep_raw_csv: list = ['game', 'model', 'experiment', 'episode', 'Aborted', 'Lose', 'Success']\n",
    "games: list = ['privateshared', 'referencegame', 'taboo', 'wordle', 'wordle_withclue', 'wordle_withcritic']  # imagegame\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T12:07:13.638452600Z",
     "start_time": "2024-07-23T12:07:13.629453200Z"
    }
   },
   "id": "caa6a02dd13f1c12"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "## Requests\n",
    "gpt_x_in: callable = lambda x: [t['content'] for t in x if t['role'] == 'user'][0]\n",
    "is_as_string: callable = lambda x: x\n",
    "inputs: callable = lambda x: x['inputs']\n",
    "prompt: callable = lambda x: x['prompt']\n",
    "\n",
    "## Responses\n",
    "completion: callable = lambda x: x['completion']\n",
    "completion_luminus: callable = lambda x: x['completions'][0]['completion']\n",
    "response: callable = lambda x: x['response']\n",
    "gpt_x_out: callable = lambda x: [t['message']['content'] for t in x['choices'] if t['message']['role'] == 'assistant']\n",
    "choices_text: callable = lambda x: x['choices'][0]['text']\n",
    "choices_content: callable = lambda x: [t['message']['content'] for t in x['choices'] if t['message']['role'] == 'assistant']\n",
    "\n",
    "text: callable = lambda x: x['text']\n",
    "content: callable = lambda x: x['content']\n",
    "\n",
    "input_output_mapping: dict = {\n",
    "    'sheep-duck-llama-2-70b-v1.1-t0.0--sheep-duck-llama-2-70b-v1.1-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'gpt-4-t0.0--gpt-3.5-turbo-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'gpt-4-1106-preview-t0.0--gpt-4-1106-preview-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'gpt-3.5-turbo-t0.0--gpt-3.5-turbo-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'luminous-supreme-t0.0--luminous-supreme-t0.0': {\n",
    "        'request': prompt,\n",
    "        'response': completion_luminus\n",
    "    },\n",
    "    'vicuna-7b-v1.5-t0.0--vicuna-7b-v1.5-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'llama-2-13b-chat-hf-t0.0--llama-2-13b-chat-hf-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': content\n",
    "    },\n",
    "    'WizardLM-70b-v1.0-t0.0--WizardLM-70b-v1.0-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'gpt-3.5-turbo-t0.0--gpt-4-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'mistral-medium-t0.0--mistral-medium-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': choices_content\n",
    "    },\n",
    "    'Mixtral-8x7B-Instruct-v0.1-t0.0--Mixtral-8x7B-Instruct-v0.1-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'koala-13b-t0.0--koala-13b-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'llama-2-7b-chat-hf-t0.0--llama-2-7b-chat-hf-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': content\n",
    "    },\n",
    "    'gpt-3.5-turbo-0613-t0.0--gpt-3.5-turbo-0613-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'gpt-4-0314-t0.0--gpt-4-0314-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'deepseek-llm-7b-chat-t0.0--deepseek-llm-7b-chat-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'koala-13B-HF-t0.0--koala-13B-HF-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'vicuna-33b-v1.3-t0.0--vicuna-33b-v1.3-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'claude-2.1-t0.0--claude-2.1-t0.0': {\n",
    "        'request': is_as_string,\n",
    "        'response': completion,\n",
    "    },\n",
    "    'Mistral-7B-Instruct-v0.1-t0.0--Mistral-7B-Instruct-v0.1-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'gpt4all-13b-snoozy-t0.0--gpt4all-13b-snoozy-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'gpt-3.5-turbo-1106-t0.0--gpt-3.5-turbo-1106-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'llama-2-70b-chat-hf-t0.0--llama-2-70b-chat-hf-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': content\n",
    "    },\n",
    "    'Wizard-Vicuna-13B-Uncensored-HF-t0.0--Wizard-Vicuna-13B-Uncensored-HF-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'openchat_3.5-t0.0--openchat_3.5-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'CodeLlama-34b-Instruct-hf-t0.0--CodeLlama-34b-Instruct-hf-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'sheep-duck-llama-2-13b-t0.0--sheep-duck-llama-2-13b-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'deepseek-llm-67b-chat-t0.0--deepseek-llm-67b-chat-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'vicuna-13b-v1.5-t0.0--vicuna-13b-v1.5-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'claude-2-t0.0--claude-2-t0.0': {\n",
    "        'request': is_as_string,\n",
    "        'response': response\n",
    "    },\n",
    "    'zephyr-7b-alpha-t0.0--zephyr-7b-alpha-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'gpt-4-0613-t0.0--gpt-4-0613-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'SUS-Chat-34B-t0.0--SUS-Chat-34B-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'tulu-2-dpo-70b-t0.0--tulu-2-dpo-70b-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'zephyr-7b-beta-t0.0--zephyr-7b-beta-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'text-davinci-003-t0.0--text-davinci-003-t0.0': {\n",
    "        'request': is_as_string,\n",
    "        'response': choices_text\n",
    "    },\n",
    "    'openchat-3.5-0106-t0.0--openchat-3.5-0106-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'Nous-Hermes-2-Mixtral-8x7B-DPO-t0.0--Nous-Hermes-2-Mixtral-8x7B-DPO-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'claude-v1.3-t0.0--claude-v1.3-t0.0': {\n",
    "        'request': is_as_string,\n",
    "        'response': completion\n",
    "    },\n",
    "    'oasst-12b-t0.0--oasst-12b-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'tulu-2-dpo-7b-t0.0--tulu-2-dpo-7b-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'openchat-3.5-1210-t0.0--openchat-3.5-1210-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'command-t0.0--command-t0.0': {\n",
    "        'request': is_as_string,\n",
    "        'response': text\n",
    "    },\n",
    "    'oasst-sft-4-pythia-12b-epoch-3.5-t0.0--oasst-sft-4-pythia-12b-epoch-3.5-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'WizardLM-13b-v1.2-t0.0--WizardLM-13b-v1.2-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'falcon-40b-t0.0--falcon-40b-t0.0': {\n",
    "        'request': is_as_string,\n",
    "        'response': response\n",
    "    },\n",
    "    'vicuna-13b-t0.0--vicuna-13b-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'gpt-4-t0.0--gpt-4-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'falcon-7b-instruct-t0.0--falcon-7b-instruct-t0.0': {\n",
    "        'request': is_as_string,\n",
    "        'response': response\n",
    "    },\n",
    "    'claude-instant-1.2-t0.0--claude-instant-1.2-t0.0': {\n",
    "        'request': is_as_string,\n",
    "        'response': response\n",
    "    },\n",
    "    'Yi-34B-Chat-t0.0--Yi-34B-Chat-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    }\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "for benchmark_version in benchmark_versions_old:\n",
    "    # read the raw_csv\n",
    "    raw_csv_data: pd.DataFrame = pd.read_csv(f'./{benchmark_version}/raw.csv')\n",
    "    \n",
    "    # group by metric to obtain all episode information\n",
    "    clean_csv_data: pd.DataFrame = group_raw_csv(data=raw_csv_data, columns_to_keep=columns_to_keep_raw_csv)\n",
    "    \n",
    "    # loop over all entries and build paths\n",
    "    for index, row in clean_csv_data.iterrows():\n",
    "        # skip image game:\n",
    "        if row.game == 'imagegame': continue\n",
    "        \n",
    "        # built paths\n",
    "        path_requests_json: str = f'./{benchmark_version}/{row.model}/{row.game}/{row.experiment}/{row.episode}/requests.json'\n",
    "        path_instance_json: str = f'./{benchmark_version}/{row.model}/{row.game}/{row.experiment}/{row.episode}/instance.json'\n",
    "\n",
    "        parse_request: callable = input_output_mapping[row.model]['request']\n",
    "        parse_response: callable = input_output_mapping[row.model]['response']\n",
    "\n",
    "        # check that paths are correctly built\n",
    "        assert os.path.isfile(path_requests_json)\n",
    "        assert os.path.isfile(path_instance_json)\n",
    "\n",
    "        prepare_requests_json(path=path_requests_json, input_parser=parse_request, output_parser=parse_response, model_name=row.model)\n",
    "\n",
    "        instance_data: dict = prepare_instance_data(path=path_instance_json)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T12:11:42.926706600Z",
     "start_time": "2024-07-23T12:11:42.109695300Z"
    }
   },
   "id": "545485eba0bc40e7"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "data: pd.DataFrame = pd.read_json('./v0.9/claude-v1.3-t0.0--claude-v1.3-t0.0/wordle/0_high_frequency_words_no_clue_no_critic/episode_0/requests.json')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T12:01:42.194604300Z",
     "start_time": "2024-07-23T12:01:42.174605500Z"
    }
   },
   "id": "9ebb7f5952dc847b"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "interactions: dict = {\n",
    "    'requests': [],\n",
    "    'responses': []\n",
    "}\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    interactions['requests'].append(row.manipulated_prompt_obj)\n",
    "    interactions['responses'].append(row.raw_response_obj['completion'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T12:05:20.904823400Z",
     "start_time": "2024-07-23T12:05:20.889824900Z"
    }
   },
   "id": "aef272366f6062b8"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "'guess: soare'"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions['responses'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T12:01:03.843220300Z",
     "start_time": "2024-07-23T12:01:03.829219100Z"
    }
   },
   "id": "1210618a0ca456c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Request format requests\n",
    "\n",
    "- claude:\n",
    "- - raw_response_obj.completion\n",
    "- Falcon:\n",
    "- - raw_response_obj.response\n",
    "- GPT 3.5/turbo GPT4\n",
    "- - manipulated_prompt_obj[role == user]\n",
    "- - raw_response_obj.choices.message.content role == assistant\n",
    "- Koala / oasst / vicuna / CodeLlama / deepseek / mistral / mixtral / Nous/ openchat / sheep / SUS / tulu / wizard / Yi / zephr\n",
    "- - manipulated_prompt_obj.inputs\n",
    "- - raw_response_obj.response\n",
    "- luminus:\n",
    "- - manipulated_prompt_obj.prompt\n",
    "- - raw_response_obj.completions.completion\n",
    "- Davinci\n",
    "- - manipulated_prompt_obj\n",
    "- - raw_response_obj.choices.text[0]\n",
    "- command:\n",
    "- - raw_request_object\n",
    "- - raw_response_obj.text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Request format interactions\n",
    "- image game\n",
    "- - \"from\": \"GM\",\n",
    "        \"to\": \"Player 1\",\n",
    "    \"from\": \"Player 1\",\n",
    "        \"to\": \"GM\",\n",
    "    \"from\": \"GM\",\n",
    "        \"to\": \"Player 2\",\n",
    "    \"from\": \"Player 2\",\n",
    "        \"to\": \"GM\","
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_output_mapping: dict = {\n",
    "    'sheep-duck-llama-2-70b-v1.1-t0.0--sheep-duck-llama-2-70b-v1.1-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'gpt-4-t0.0--gpt-3.5-turbo-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'gpt-4-1106-preview-t0.0--gpt-4-1106-preview-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'gpt-3.5-turbo-t0.0--gpt-3.5-turbo-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'luminous-supreme-t0.0--luminous-supreme-t0.0': {\n",
    "        'request': prompt,\n",
    "        'response': completion\n",
    "    },\n",
    "    'vicuna-7b-v1.5-t0.0--vicuna-7b-v1.5-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'llama-2-13b-chat-hf-t0.0--llama-2-13b-chat-hf-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'WizardLM-70b-v1.0-t0.0--WizardLM-70b-v1.0-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'gpt-3.5-turbo-t0.0--gpt-4-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'mistral-medium-t0.0--mistral-medium-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'Mixtral-8x7B-Instruct-v0.1-t0.0--Mixtral-8x7B-Instruct-v0.1-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'koala-13b-t0.0--koala-13b-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'llama-2-7b-chat-hf-t0.0--llama-2-7b-chat-hf-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'gpt-3.5-turbo-0613-t0.0--gpt-3.5-turbo-0613-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'gpt-4-0314-t0.0--gpt-4-0314-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'deepseek-llm-7b-chat-t0.0--deepseek-llm-7b-chat-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'koala-13B-HF-t0.0--koala-13B-HF-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'vicuna-33b-v1.3-t0.0--vicuna-33b-v1.3-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'claude-2.1-t0.0--claude-2.1-t0.0': {\n",
    "        'request': is_as_string,\n",
    "        'response': completion\n",
    "    },\n",
    "    'Mistral-7B-Instruct-v0.1-t0.0--Mistral-7B-Instruct-v0.1-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'gpt4all-13b-snoozy-t0.0--gpt4all-13b-snoozy-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'gpt-3.5-turbo-1106-t0.0--gpt-3.5-turbo-1106-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'llama-2-70b-chat-hf-t0.0--llama-2-70b-chat-hf-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'Wizard-Vicuna-13B-Uncensored-HF-t0.0--Wizard-Vicuna-13B-Uncensored-HF-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'openchat_3.5-t0.0--openchat_3.5-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'CodeLlama-34b-Instruct-hf-t0.0--CodeLlama-34b-Instruct-hf-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'sheep-duck-llama-2-13b-t0.0--sheep-duck-llama-2-13b-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'deepseek-llm-67b-chat-t0.0--deepseek-llm-67b-chat-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'vicuna-13b-v1.5-t0.0--vicuna-13b-v1.5-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'claude-2-t0.0--claude-2-t0.0': {\n",
    "        'request': is_as_string,\n",
    "        'response': completion\n",
    "    },\n",
    "    'zephyr-7b-alpha-t0.0--zephyr-7b-alpha-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'gpt-4-0613-t0.0--gpt-4-0613-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'SUS-Chat-34B-t0.0--SUS-Chat-34B-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'tulu-2-dpo-70b-t0.0--tulu-2-dpo-70b-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'zephyr-7b-beta-t0.0--zephyr-7b-beta-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'text-davinci-003-t0.0--text-davinci-003-t0.0': {\n",
    "        'request': is_as_string,\n",
    "        'response': choices_text\n",
    "    },\n",
    "    'openchat-3.5-0106-t0.0--openchat-3.5-0106-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'Nous-Hermes-2-Mixtral-8x7B-DPO-t0.0--Nous-Hermes-2-Mixtral-8x7B-DPO-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'claude-v1.3-t0.0--claude-v1.3-t0.0': {\n",
    "        'request': is_as_string,\n",
    "        'response': completion\n",
    "    },\n",
    "    'oasst-12b-t0.0--oasst-12b-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'tulu-2-dpo-7b-t0.0--tulu-2-dpo-7b-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'openchat-3.5-1210-t0.0--openchat-3.5-1210-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'command-t0.0--command-t0.0': {\n",
    "        'request': is_as_string,\n",
    "        'response': text\n",
    "    },\n",
    "    'oasst-sft-4-pythia-12b-epoch-3.5-t0.0--oasst-sft-4-pythia-12b-epoch-3.5-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'WizardLM-13b-v1.2-t0.0--WizardLM-13b-v1.2-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'falcon-40b-t0.0--falcon-40b-t0.0': {\n",
    "        'request': is_as_string,\n",
    "        'response': response\n",
    "    },\n",
    "    'vicuna-13b-t0.0--vicuna-13b-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    },\n",
    "    'gpt-4-t0.0--gpt-4-t0.0': {\n",
    "        'request': gpt_x_in,\n",
    "        'response': gpt_x_out\n",
    "    },\n",
    "    'falcon-7b-instruct-t0.0--falcon-7b-instruct-t0.0': {\n",
    "        'request': is_as_string,\n",
    "        'response': response\n",
    "    },\n",
    "    'claude-instant-1.2-t0.0--claude-instant-1.2-t0.0': {\n",
    "        'request': is_as_string,\n",
    "        'response': completion\n",
    "    },\n",
    "    'Yi-34B-Chat-t0.0--Yi-34B-Chat-t0.0': {\n",
    "        'request': inputs,\n",
    "        'response': response\n",
    "    }\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
